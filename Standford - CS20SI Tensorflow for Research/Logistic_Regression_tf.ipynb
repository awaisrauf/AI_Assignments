{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/data/mnist',one_hot=True)\n",
    "X_train,Y_train = mnist.train.next_batch(60000)\n",
    "X_test,Y_test = mnist.test.next_batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train=60000\n",
    "num_test =10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch_RAM(X,Y,batch_size,batch_number):\n",
    "    num_classes = 10\n",
    "    # Indices to slice data into batches\n",
    "    start = batch_number*batch_size\n",
    "    end = start + batch_size\n",
    "    batch_X = X[start:end,:]\n",
    "    batch_Y = Y[start:end,:]\n",
    "    return batch_X,batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defing Model\n",
    "# Define parameters \n",
    "tf.reset_default_graph()\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "lamda = 0.0001\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    #Step 2: \n",
    "    X = tf.placeholder(tf.float32,[batch_size,784],name='X_placeholder')\n",
    "    Y = tf.placeholder(tf.int32,[batch_size,10],name='Y')\n",
    "    #Step 3:\n",
    "    weights = tf.Variable(tf.random_normal(shape=[784,10],stddev=0.01),name='weights') \n",
    "    bias = tf.Variable(tf.zeros([1,10]),name='bias')\n",
    "\n",
    "    #Step 4:\n",
    "    logits = tf.matmul(X,weights) + bias\n",
    "    \n",
    "    #Step 5: \n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=logits,name='loss')\n",
    "    loss = tf.reduce_mean(entropy) + lamda*tf.norm(weights,ord=2)\n",
    "    # Step 6:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################Average loss epoch 0:0.5744087563620673\n",
      "###############################################Average loss epoch 1:0.37557274864142776\n",
      "###############################################Average loss epoch 2:0.3438213505487666\n",
      "###############################################Average loss epoch 3:0.3272504346747684\n",
      "###############################################Average loss epoch 4:0.3165420026032843\n",
      "###############################################Average loss epoch 5:0.30885641668469477\n",
      "###############################################Average loss epoch 6:0.3029764636268473\n",
      "###############################################Average loss epoch 7:0.2982781028263589\n",
      "###############################################Average loss epoch 8:0.294403268613367\n",
      "###############################################Average loss epoch 9:0.2911297579924775\n",
      "###############################################Average loss epoch 10:0.2883114574684037\n",
      "###############################################Average loss epoch 11:0.28584780499466467\n",
      "###############################################Average loss epoch 12:0.2836670415141644\n",
      "###############################################Average loss epoch 13:0.28171637496696067\n",
      "###############################################Average loss epoch 14:0.2799559906284269\n",
      "###############################################Average loss epoch 15:0.27835521171999794\n",
      "###############################################Average loss epoch 16:0.2768899478718766\n",
      "###############################################Average loss epoch 17:0.27554098344766176\n",
      "###############################################Average loss epoch 18:0.27429276123706603\n",
      "###############################################Average loss epoch 19:0.27313254741776705\n",
      "###############################################Average loss epoch 20:0.27204977533119357\n",
      "###############################################Average loss epoch 21:0.2710356188731061\n",
      "###############################################Average loss epoch 22:0.27008261122446287\n",
      "###############################################Average loss epoch 23:0.2691844006220245\n",
      "###############################################Average loss epoch 24:0.2683355448465062\n",
      "###############################################Average loss epoch 25:0.2675313292405544\n",
      "###############################################Average loss epoch 26:0.26676768565980286\n",
      "###############################################Average loss epoch 27:0.26604103203066903\n",
      "###############################################Average loss epoch 28:0.26534824031922555\n",
      "###############################################Average loss epoch 29:0.26468654096317595\n",
      "###############################################Average loss epoch 30:0.26405348515727073\n",
      "###############################################Average loss epoch 31:0.26344688032936847\n",
      "###############################################Average loss epoch 32:0.2628647844888206\n",
      "###############################################Average loss epoch 33:0.26230544660590654\n",
      "###############################################Average loss epoch 34:0.26176728922714537\n",
      "###############################################Average loss epoch 35:0.2612488958029411\n",
      "###############################################Average loss epoch 36:0.26074897243171674\n",
      "###############################################Average loss epoch 37:0.2602663508688028\n",
      "###############################################Average loss epoch 38:0.2597999719855113\n",
      "###############################################Average loss epoch 39:0.2593488517161618\n",
      "###############################################Average loss epoch 40:0.25891210385558444\n",
      "###############################################Average loss epoch 41:0.2584889145074492\n",
      "###############################################Average loss epoch 42:0.2580785268010237\n",
      "###############################################Average loss epoch 43:0.257680252687926\n",
      "###############################################Average loss epoch 44:0.257293453010229\n",
      "###############################################Average loss epoch 45:0.25691753274036777\n",
      "###############################################Average loss epoch 46:0.2565519417771417\n",
      "###############################################Average loss epoch 47:0.25619617615563745\n",
      "###############################################Average loss epoch 48:0.25584976566143525\n",
      "###############################################Average loss epoch 49:0.2555122643741023\n",
      "###############################################Average loss epoch 50:0.25518326736731917\n",
      "###############################################Average loss epoch 51:0.25486238811833734\n",
      "###############################################Average loss epoch 52:0.25454926282231116\n",
      "###############################################Average loss epoch 53:0.2542435612497676\n",
      "###############################################Average loss epoch 54:0.2539449630575812\n",
      "###############################################Average loss epoch 55:0.2536531840252061\n",
      "###############################################Average loss epoch 56:0.25336793313423794\n",
      "###############################################Average loss epoch 57:0.25308895408788806\n",
      "###############################################Average loss epoch 58:0.25281599337537575\n",
      "###############################################Average loss epoch 59:0.2525488300901702\n",
      "###############################################Average loss epoch 60:0.2522872408899741\n",
      "###############################################Average loss epoch 61:0.2520310140700422\n",
      "###############################################Average loss epoch 62:0.25177994978606194\n",
      "###############################################Average loss epoch 63:0.2515338688260979\n",
      "###############################################Average loss epoch 64:0.2512925914846934\n",
      "###############################################Average loss epoch 65:0.25105594800641906\n",
      "###############################################Average loss epoch 66:0.25082378587725324\n",
      "###############################################Average loss epoch 67:0.25059594288786763\n",
      "###############################################Average loss epoch 68:0.2503722861058946\n",
      "###############################################Average loss epoch 69:0.25015267139125585\n",
      "###############################################Average loss epoch 70:0.2499369749815291\n",
      "###############################################Average loss epoch 71:0.24972506994620347\n",
      "###############################################Average loss epoch 72:0.2495168279697243\n",
      "###############################################Average loss epoch 73:0.24931214837373322\n",
      "###############################################Average loss epoch 74:0.24911091395486623\n",
      "###############################################Average loss epoch 75:0.24891303045054278\n",
      "###############################################Average loss epoch 76:0.2487183933138338\n",
      "###############################################Average loss epoch 77:0.2485269046205486\n",
      "###############################################Average loss epoch 78:0.2483384748841198\n",
      "###############################################Average loss epoch 79:0.24815301702190667\n",
      "###############################################Average loss epoch 80:0.2479704541878568\n",
      "###############################################Average loss epoch 81:0.24779070334302056\n",
      "###############################################Average loss epoch 82:0.24761368189420965\n",
      "###############################################Average loss epoch 83:0.24743932399612206\n",
      "###############################################Average loss epoch 84:0.24726755334398684\n",
      "###############################################Average loss epoch 85:0.24709830802475286\n",
      "###############################################Average loss epoch 86:0.24693150797651875\n",
      "###############################################Average loss epoch 87:0.24676710131586108\n",
      "###############################################Average loss epoch 88:0.24660502839037496\n",
      "###############################################Average loss epoch 89:0.2464452272392491\n",
      "###############################################Average loss epoch 90:0.24628764355921337\n",
      "###############################################Average loss epoch 91:0.24613221612177852\n",
      "###############################################Average loss epoch 92:0.24597889695985195\n",
      "###############################################Average loss epoch 93:0.24582763821778134\n",
      "###############################################Average loss epoch 94:0.24567838933350694\n",
      "###############################################Average loss epoch 95:0.24553110366129977\n",
      "###############################################Average loss epoch 96:0.24538573133958202\n",
      "###############################################Average loss epoch 97:0.24524222737830928\n",
      "###############################################Average loss epoch 98:0.24510056297812197\n",
      "###############################################Average loss epoch 99:0.24496068299198762\n",
      "Total_time:{0} seconds 132.4793803691864\n",
      "Accuracy:0.9245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "# Parameters and variables\n",
    "\n",
    "# Number of batches for test and train set\n",
    "n_batches_train = int(math.floor(num_train/batch_size))\n",
    "n_batches_test = int(math.floor(num_test/batch_size)) \n",
    "#print(n_batches_train)\n",
    "# Run Session\n",
    "with tf.Session(graph=graph1) as sess:\n",
    "    #writer = tf.summary.FileWriter('./graphs/assignment1_b',sess.graph)\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for j in range(n_batches_train):\n",
    "            batch_number = j\n",
    "            X_batch,Y_batch = get_next_batch_RAM(X_train,Y_train,batch_size,batch_number)\n",
    "            _,loss_batch = sess.run([optimizer,loss],feed_dict={X:X_batch,Y:Y_batch})\n",
    "            total_loss +=loss_batch\n",
    "            # Prints one hash after 10 batches\n",
    "            if (j%10==0):\n",
    "                print('#', end = \"\")\n",
    "        print('Average loss epoch {0}:{1}'.format(i,total_loss/n_batches_train))    \n",
    "    print('Total_time:{0} seconds',format(time.time()-start_time))\n",
    "    \n",
    "# Test The model\n",
    "    n_batches = math.floor((num_test)/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches_test):\n",
    "        #print(i,end='')\n",
    "        batch_number = i\n",
    "        X_batch,Y_batch = get_next_batch_RAM(X_test,Y_test,batch_size,batch_number)\n",
    "        #loss_batch,logits_batch = sess.run([loss,logits],feed_dict={X:X_batch,Y:Y_batch})\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        correct_preds = tf.equal(tf.argmax(probs,1),tf.argmax(Y_batch,1))\n",
    "        n_correct_preds = tf.reduce_sum(tf.cast(correct_preds,tf.float32))\n",
    "        total_correct_preds += sess.run(n_correct_preds,feed_dict={X:X_batch,Y:Y_batch})\n",
    "    print('Accuracy:{0}'.format(total_correct_preds/num_test))  \n",
    "    #print('Loss{0}'.format(loss_total)) \n",
    "#writer.close()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/data/mnist',one_hot=False)\n",
    "X_train,Y_train = mnist.train.next_batch(60000)\n",
    "X_test,Y_test = mnist.test.next_batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "LR = linear_model.LogisticRegression(max_iter=100)\n",
    "LR.fit(X_train,Y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(Y_test, y_pred, sample_weight=None)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
