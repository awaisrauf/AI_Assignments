{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"ModelSelection_TensorBoard.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"E7MhtI53rITp","colab_type":"text"},"source":["# Use of Tensorboard "]},{"cell_type":"markdown","metadata":{"id":"EWxWTQqPrITr","colab_type":"text"},"source":["This notebook provides a basic structure to use tensorboard for visulization of network graph and other important features. Use command tensorboard --logdir=graphs to get a feel of this notebook tensorboard. \n","\\"]},{"cell_type":"code","metadata":{"id":"yFfwaCDdrITs","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import xlrd\n","import math\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zxo0NC_frITx","colab_type":"code","colab":{},"outputId":"3c5b4ba7-fb13-4cba-f3cd-ae656c287bf0"},"source":["# Step 1: Read in Data\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets('/data/mnist',one_hot=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Extracting /data/mnist\\train-images-idx3-ubyte.gz\n","Extracting /data/mnist\\train-labels-idx1-ubyte.gz\n","Extracting /data/mnist\\t10k-images-idx3-ubyte.gz\n","Extracting /data/mnist\\t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yJWnCVIqrIT4","colab_type":"code","colab":{}},"source":["run_var=0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Xs4TRxLrIT7","colab_type":"code","colab":{}},"source":["# To avoid haveing different graphs colliding\n","tf.reset_default_graph()\n","learning_rate = 0.001\n","batch_size = 128\n","n_epochs = 1\n","lamda = 0.0001\n","weight_initializer_xavier = tf.contrib.layers.xavier_initializer()\n","# Neurons in each layer\n","X_shape_0 = 784 # X.shape[0]\n","n_outputs = 10\n","Neurons_1 = 100\n","Neurons_2 = 50\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cls4a723rIUA","colab_type":"code","colab":{}},"source":["def NN_Layer(input_1,size_in,size_out,name='Layer',activation='relu'):\n","    with tf.name_scope(name):\n","        w = tf.Variable(weight_initializer_xavier(shape=[size_in,size_out]),name='w') \n","        b = tf.Variable(tf.zeros([1,size_out]),name='b')\n","        z = tf.matmul(input_1,w) + b\n","        if(activation=='relu'):\n","            a = tf.nn.relu(z)\n","        elif(activation=='sigmoid'):\n","            a = tf.nn.sigmoid(z)\n","        else:\n","            print('Error: Define Activation function again, carefully!')\n","        tf.summary.histogram(\"weights\", w)\n","        tf.summary.histogram(\"biases\", b)\n","        tf.summary.histogram(\"activations\", a)\n","        return a\n","    \n","\n","# function creates graph of NN\n","def NeuralNets(learning_rate):    \n","    #plcae holders for input: \n","    X = tf.placeholder(tf.float32,[None,X_shape_0],name='X_placeholder')\n","    Y = tf.placeholder(tf.float32,[None,n_outputs],name='Y_placeholder')\n","    x_image=tf.reshape(X, [-1, 28, 28, 1])\n","    tf.summary.image('input', x_image, 3)\n","    # Layer 1\n","    layer_1 = NN_Layer(X,X_shape_0,Neurons_1,name='Layer_1')\n","    # Layer 2\n","    layer_2 = NN_Layer(layer_1,Neurons_1,Neurons_2,name='Layer_2') \n","    #Output Layer\n","    layer_last = NN_Layer(layer_2,Neurons_2,n_outputs,name='Layer_last',activation='sigmoid')\n","                                   \n","    \n","    # specify cost function\n","    with tf.name_scope('entropy'):\n","    # this is our cost\n","        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=layer_last,name='entropy')\n","        loss = tf.reduce_mean(entropy)\n","    # specify optimizer\n","    with tf.name_scope('train'):\n","        # optimizer is an \"operation\" which we can execute in a session\n","        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","\n","    with tf.name_scope('Accuracy'):\n","        # Accuracy\n","        correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(layer_last,1))\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","                                   \n","    # create a summary for our cost and accuracy\n","    tf.summary.scalar(\"cost\", loss)\n","    tf.summary.scalar(\"accuracy\", accuracy)                               \n","                                                                                          \n","    summary_op = tf.summary.merge_all()\n","    return summary_op,optimizer,loss    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"se5pojW9rIUH","colab_type":"code","colab":{},"outputId":"0d2b4f77-4a0a-4215-c37d-eaee1eb43ef7"},"source":["# Run Session\n","log_dir = './graphs/TensorboardTUT/Adam/'\n","tf.reset_default_graph()\n","#Define graph\n","graph1 = tf.Graph()\n","with graph1.as_default():\n","    summary_op,optimizer,loss = NeuralNets(learning_rate)\n","    with tf.Session(graph=graph1) as sess:\n","        # to save the graph\n","        #writer = tf.summary.FileWriter('./graphs/TensorboardTUT/',sess.graph)\n","        writer = tf.summary.FileWriter('%s/%s' % (log_dir, run_var), sess.graph)\n","        #start_time = time.time()\n","        sess.run(tf.global_variables_initializer())\n","        n_batches = int(mnist.train.num_examples/batch_size)\n","\n","        #Training \n","        for epoch in range(n_epochs):\n","            total_loss = 0\n","            # Passing through each batch\n","            for j in range(n_batches):\n","                X_batch,Y_batch = mnist.train.next_batch(batch_size)\n","                _,loss_batch,summary = sess.run([optimizer,loss,summary_op],feed_dict={X:X_batch,Y:Y_batch})\n","                total_loss +=loss_batch\n","                writer.add_summary(summary, epoch * n_batches + j)\n","\n","\n","            print('Average loss epoch {0}:{1}'.format(epoch,total_loss/n_batches))    \n","        #print('Total_time:{0} seconds',format(time.time()-start_time))\n","\n","        # Testing\n","        n_batches = int(mnist.test.num_examples/batch_size)\n","        total_correct_preds = 0\n","        for i in range(n_batches):\n","            X_batch, Y_batch = mnist.test.next_batch(batch_size)\n","            accuracy_batch = sess.run(accuracy,feed_dict={X:X_batch,Y:Y_batch})\n","            total_correct_preds += accuracy_batch\n","        print('Accuracy{0}'.format(total_correct_preds))    #/mnist.test.num_examples\n","        run_var+=1\n","    writer.close()    "],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-14-45550c5cb343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"]}]},{"cell_type":"code","metadata":{"id":"KUbj5_pPrIUM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDuqijVxrIUP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}