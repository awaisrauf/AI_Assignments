{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asignment 1b Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1 Question 2 of stanford course, Tensorflow for Deep Learning Research. \n",
    "Assignment available here https://web.stanford.edu/class/cs20si/2017/assignments/a1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 Task 3\n",
    " Build a logistic regression model to predict whether someone has coronary heart disease\n",
    "In the data folder on the class’s GitHub repo, you’ll find the file heart.txt. The first row is the\n",
    "name of the observed variables. There are 10 variables:\n",
    "- sbp: Systolic blood pressure\n",
    "- tobacco: Cumulative tobacco consumption, in kg\n",
    "- ldl: Low-density lipoprotein cholesterol\n",
    "- adiposity: Adipose tissue concentration\n",
    "- famhist: Family history of heart disease (1=Present, 0=Absent)\n",
    "- typea: Score on test designed to measure type-A behavior\n",
    "- obesity: Obesity\n",
    "- alcohol: Current consumption of alcohol\n",
    "- age: Age of subject\n",
    "- chd: Coronary heart disease at baseline; 1=Yes 0=No\n",
    "\n",
    "Each following row contains the information of one patient. There are 462 samples in total.\n",
    "We will be using the first 9 variables to predict the last variable. That is, your input will be 1-d\n",
    "tensor of 9 elements, and your label is binary. You should write the function to read in data\n",
    "yourself, and you should take care of dividing your data into train set and test set.\n",
    "Report your results and your hyperparameters.\n",
    "Source of the data:\n",
    "http://statweb.stanford.edu/~tibs/ElemStatLearn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_data = genfromtxt('heart.csv', delimiter=',')\n",
    "data_np = my_data[1:463,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Shuffle and divide into train and test\n",
    "data_np = shuffle(data_np)\n",
    "data_np = np.asarray(data_np)\n",
    "\n",
    "X_train = data_np[47:-1,0:9].astype(float)\n",
    "Y_train = data_np[47:-1,9]\n",
    "Y_train = Y_train.reshape(-1,1)#.astype(float)\n",
    "X_test = data_np [0:47,0:9].astype(float)\n",
    "Y_test = data_np[0:47,9]\n",
    "Y_test=Y_test.reshape(-1,1)#.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defing Model\n",
    "# Define parameters \n",
    "tf.reset_default_graph()\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10000\n",
    "lamda = 0.1\n",
    "LRHeart = tf.Graph()\n",
    "with LRHeart.as_default():\n",
    "    #Step 2: \n",
    "    X = tf.placeholder(tf.float32,[None,9],name='X')\n",
    "    Y = tf.placeholder(tf.int32,[None,1],name='Y')\n",
    "    #Step 3:\n",
    "    weights = tf.Variable(tf.random_normal(shape=[9,1],stddev=0.001),name='weights') \n",
    "    bias = tf.Variable(tf.zeros([1]),name='bias')\n",
    "\n",
    "    #Step 4:\n",
    "    logits = tf.matmul(X,weights) + bias\n",
    "    \n",
    "    #Step 5: \n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=logits,name='loss')\n",
    "    loss = tf.reduce_mean(entropy)+ lamda*tf.norm(weights,ord=2)\n",
    "    # Step 6:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)  \n",
    "    # Test \n",
    "    probs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 9999:0.24593539399225356\n",
      "Total_time:{0} seconds 5.269111156463623\n",
      "\n",
      "Accuracy:0.3617021276595745\n"
     ]
    }
   ],
   "source": [
    "# Parameters and variables\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# Run Session\n",
    "with tf.Session(graph=LRHeart) as sess:\n",
    "    #writer = tf.summary.FileWriter('./graphs/assignment1_b',sess.graph)\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(n_epochs):\n",
    "        _,loss_1 = sess.run([optimizer,loss],feed_dict={X:X_train,Y:Y_train})\n",
    "\n",
    "        # Prints one hash after 10 batches\n",
    "    print('Average loss epoch {0}:{1}'.format(i,loss_1/n_train))    \n",
    "    print('Total_time:{0} seconds',format(time.time()-start_time))\n",
    "    \n",
    "# Test The model\n",
    "    probs_1 = sess.run(probs,feed_dict={X:X_test})\n",
    "    accuracy_batch = tf.equal(probs_1,Y_test)\n",
    "    correct_preds_batch= tf.reduce_sum(tf.cast(accuracy_batch,tf.float32))\n",
    "    accuracy=sess.run(correct_preds_batch)   \n",
    "print('\\nAccuracy:{0}'.format(accuracy/n_test))  \n",
    "#writer.close()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Sickit Learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723404255319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "LR = linear_model.LogisticRegression(max_iter=1000)\n",
    "LR.fit(X_train,Y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(Y_test, y_pred, sample_weight=None)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Accuracy with tensorflow(40.43) is way less than scikit learn(68.09). </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Better Logistic Model in tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_var=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defing Model\n",
    "# Define parameters \n",
    "tf.reset_default_graph()\n",
    "#learning_rate = 0.01\n",
    "n_epochs = 1000\n",
    "#lamda = 0.001\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "def LogisticRegression(loss_type,lamda,learning_rate,Optimizer=tf.train.GradientDescentOptimizer):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32,[None,9],name='X')\n",
    "    Y = tf.placeholder(tf.float32,[None,1],name='Y')\n",
    "    #:\n",
    "    with tf.name_scope('weight'):\n",
    "        weights = tf.Variable(initializer(shape=[9,1]),name='weights')\n",
    "        tf.summary.histogram(\"weights\", weights)\n",
    "       \n",
    "    with tf.name_scope('bias'):\n",
    "        bias = tf.Variable(tf.zeros([1]),name='bias')\n",
    "        tf.summary.histogram(\"biases\", bias)\n",
    "\n",
    "        #Step 4:\n",
    "    with tf.name_scope('logit'):\n",
    "        logits = tf.matmul(X,weights) + bias \n",
    "        tf.summary.histogram(\"activations\", logits)\n",
    "\n",
    "    #Step 5: \n",
    "    with tf.name_scope('entropy'):\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=logits,name='loss')\n",
    "        tf.summary.scalar('entropy',entropy)\n",
    "        loss = tf.reduce_mean(entropy)    \n",
    "        tf.summary.scalar('loss',loss)\n",
    "        \n",
    "        # Step 6:\n",
    "    with tf.name_scope('Train'):\n",
    "        optimizer = Optimizer(learning_rate).minimize(loss)  \n",
    "        # Test \n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Accuracy\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        accuracy_batch = tf.equal(probs,Y)\n",
    "        accuracy= tf.reduce_mean(tf.cast(accuracy_batch,tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    validation_acc = tf.summary.scalar(\"Validation_Accuracy\",accuracy)     \n",
    "    #return optimizer,loss\n",
    "    #to save the graph\n",
    "    #writer = tf.summary.FileWriter('./graphs/TensorboardTUT/',sess.graph)\n",
    "    sess = tf.Session()\n",
    "    writer = tf.summary.FileWriter('%s/%s' % (log_dir, run_var), sess.graph)\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    #Training \n",
    "    accuracy_test_np=0\n",
    "    for epoch in range(n_epochs):\n",
    "        # Passing through each batch\n",
    "        print(epoch,end='')\n",
    "        _,loss_batch,summary= sess.run([optimizer,loss,summary_op],feed_dict={X:X_train,Y:Y_train}) #,summary ,summary_op\n",
    "        writer.add_summary(summary, epoch )\n",
    "        #summary_validation = sess.run(validation_acc,feed_dict={X:X_test,Y:Y_test})\n",
    "        #writer.add_summary(summary_validation,epoch)\n",
    "        #accuracy_test = sess.run(accuracy,feed_dict={X:X_test,Y:Y_test})\n",
    "        #accuracy_test_np =accuracy_test \n",
    "        print('Average loss epoch {0}:{1}'.format(epoch,loss_batch,accuracy_test_np))          \n",
    "    writer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "tags and values not the same shape: [] != [414] (tag 'entropy/entropy')\n\t [[Node: entropy/entropy = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](entropy/entropy/tags, entropy/Reshape_2)]]\n\nCaused by op 'entropy/entropy', defined at:\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-27-088435abbf4c>\", line 6, in <module>\n    LogisticRegression(loss_type='DRAC1',lamda=0.1,learning_rate=0.01,Optimizer=tf.train.GradientDescentOptimizer)\n  File \"<ipython-input-26-ed19cba643f0>\", line 31, in LogisticRegression\n    tf.summary.scalar('entropy',entropy)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\summary\\summary.py\", line 129, in scalar\n    tags=scope.rstrip('/'), values=tensor, name=scope)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 265, in _scalar_summary\n    name=name)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [414] (tag 'entropy/entropy')\n\t [[Node: entropy/entropy = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](entropy/entropy/tags, entropy/Reshape_2)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: tags and values not the same shape: [] != [414] (tag 'entropy/entropy')\n\t [[Node: entropy/entropy = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](entropy/entropy/tags, entropy/Reshape_2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-088435abbf4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./graphs/Assignment1/Question2c/try2/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Run Session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'DRAC1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlamda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#with tf.Session(graph=graph1) as sess:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#    sess.run(tf.global_variables_initializer())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-ed19cba643f0>\u001b[0m in \u001b[0;36mLogisticRegression\u001b[1;34m(loss_type, lamda, learning_rate, Optimizer)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Passing through each batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#,summary ,summary_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m#summary_validation = sess.run(validation_acc,feed_dict={X:X_test,Y:Y_test})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: tags and values not the same shape: [] != [414] (tag 'entropy/entropy')\n\t [[Node: entropy/entropy = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](entropy/entropy/tags, entropy/Reshape_2)]]\n\nCaused by op 'entropy/entropy', defined at:\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-27-088435abbf4c>\", line 6, in <module>\n    LogisticRegression(loss_type='DRAC1',lamda=0.1,learning_rate=0.01,Optimizer=tf.train.GradientDescentOptimizer)\n  File \"<ipython-input-26-ed19cba643f0>\", line 31, in LogisticRegression\n    tf.summary.scalar('entropy',entropy)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\summary\\summary.py\", line 129, in scalar\n    tags=scope.rstrip('/'), values=tensor, name=scope)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 265, in _scalar_summary\n    name=name)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\ss\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [414] (tag 'entropy/entropy')\n\t [[Node: entropy/entropy = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](entropy/entropy/tags, entropy/Reshape_2)]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters and variables\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "log_dir = './graphs/Assignment1/Question2c/try2/'\n",
    "# Run Session\n",
    "LogisticRegression(loss_type='DRAC1',lamda=0.1,learning_rate=0.01,Optimizer=tf.train.GradientDescentOptimizer)\n",
    "#with tf.Session(graph=graph1) as sess:\n",
    "#    sess.run(tf.global_variables_initializer())  \n",
    "#    _,loss_batch,summary = sess.run([optimizer,loss],feed_dict={X:X_train,Y:Y_train})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-cea6f8be4ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./graphs/Assignment1/Question2c/try1/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# to save the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#writer = tf.summary.FileWriter('./graphs/TensorboardTUT/',sess.graph)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graph1' is not defined"
     ]
    }
   ],
   "source": [
    "# Run Session\n",
    "log_dir = './graphs/Assignment1/Question2c/try1/'\n",
    "tf.reset_default_graph()\n",
    "with tf.Session(graph=graph1) as sess:\n",
    "    # to save the graph\n",
    "    #writer = tf.summary.FileWriter('./graphs/TensorboardTUT/',sess.graph)\n",
    "    writer = tf.summary.FileWriter('%s/%s' % (log_dir, run_var), sess.graph)\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    #Training \n",
    "    for epoch in range(n_epochs):\n",
    "        # Passing through each batch\n",
    "        _,loss_batch,summary = sess.run([optimizer,loss,summary_op],feed_dict={X:X_train,Y:Y_train})\n",
    "        writer.add_summary(summary, epoch )\n",
    "        summary_validatioin = sess.run(validation_acc)\n",
    "        #writer.add_summary(summary_validation,epoch)\n",
    "        #print('Average loss epoch {0}:{1}'.format(epoch,loss_batch)        \n",
    "    \n",
    "    writer.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Different loss functions    \n",
    "        if (loss_type=='simple'):\n",
    "            with tf.name_scope('loss'):\n",
    "                loss = tf.reduce_mean(entropy)\n",
    "        elif (loss_type=='l1'):\n",
    "            with tf.name_scope('loss'):\n",
    "                loss = tf.reduce_mean(entropy)+ lamda*tf.norm(weights,ord=1)\n",
    "        elif (loss_type=='l2'):\n",
    "            with tf.name_scope('loss'):\n",
    "                loss = tf.reduce_mean(entropy)+ lamda*tf.norm(weights,ord=2)    \n",
    "        elif (loss_type=='linf'):\n",
    "            with tf.name_scope('loss'):\n",
    "                loss = tf.reduce_mean(entropy)+ lamda*tf.norm(weights,ord=np.inf)\n",
    "        elif (loss_type=='DRAC1'):\n",
    "            with tf.name_scope('loss'):\n",
    "                loss = tf.reduce_mean(entropy)+ lamda*tf.norm(weights,ord=np.inf)         \n",
    "        else: \n",
    "            print('Revise loss_type, only accepts: simple,l1,l2,linf,DRAC1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
