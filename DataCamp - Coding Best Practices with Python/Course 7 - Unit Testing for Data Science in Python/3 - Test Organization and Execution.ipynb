{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Organization and Execution\n",
    "## Test Directory Structure \n",
    "- inside the tests folder, mirror whole package as shown in the figure. \n",
    "![](imgs/1.png)\n",
    "- for tests each function, create one class holding all the test functions.\n",
    "```python\n",
    "class TestFunctionName(object):\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line to Run Tests\n",
    "- to run all tests\n",
    "```bash\n",
    "cd tests\n",
    "pytest\n",
    "```\n",
    "- before pushing, sometimes we only want to know if all test pass oro fail. In this case, it is better to stop if any test fails. To test this, we use\n",
    "```bash\n",
    "pytest -x\n",
    "```\n",
    "- to run one test class\n",
    "```bash\n",
    "pytest folder::test_file_name.py::TestClassName\n",
    "```\n",
    "- to test one test function inside the class\n",
    "```bash\n",
    "pytest folder/test_file_name.py::TestClassName::test_function_name\n",
    "```\n",
    "\n",
    "- to test only pharases that match a specific name\n",
    "```bash\n",
    "pytest -k expression-to-match\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected to Fail\n",
    "- if we expect a test to fail, we can use `@pytest.mark.xfail` as follows;\n",
    "    ```python\n",
    "    import pytest\n",
    "\n",
    "    # whole test class will be skipped\n",
    "    @pytest.mark.xfail\n",
    "    class TestABC(object):\n",
    "        pass\n",
    "\n",
    "    ```\n",
    "    - to skip a function \n",
    "    \n",
    "    ```python\n",
    "    class TestABC(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            pass\n",
    "        \n",
    "        @pytest.mark.xfail\n",
    "        def test_a_function(self):\n",
    "            pass\n",
    "    ```\n",
    "    \n",
    "- can also add reason for the failure\n",
    "\n",
    "    ```python\n",
    "    @pytest.mark.xfail(reason=\"test has not been implemented yet\")\n",
    "    class TestABC(object):\n",
    "        pass\n",
    "    ```\n",
    "    \n",
    "- can skip tests conditionally \n",
    "```python\n",
    "    @pytest.mark.xskipif(sys.version > (2, 7))\n",
    "    def test_function():\n",
    "        pass\n",
    "```\n",
    "\n",
    "- to get only the reason of the test failure \n",
    "```bash\n",
    "pytest -rs models/test_train.py\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous Intergration of Test\n",
    "- To run tests automatically whenever we push into github, use Travis CI\n",
    "#### Step 1: Create a file `.travis.yml` in the root directory and populate it with following:\n",
    "![](imgs/2.png)\n",
    "#### Step 2: Install it on github marketplace\n",
    "#### Step 3: Open TravisCI dashboard \n",
    "- we can also get badage \n",
    "![](imgs/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Coverage\n",
    "To enable the code coverage test from CodeCoverage\n",
    "1. add following in the `.travis.yml` file.\n",
    "![](imgs/4.png)\n",
    "2. go to github marketplace and install CodeCoverage\n",
    "3. Get markdown for the badge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
