Deep learning can do most tasks that consists of mapping input to output vector which a person can do rapidly given large enough data and model.

# Deep Feedforward Neural Networks 

Feedforward neural networks are function approximaters consisting of many hidden layers. Given labelled data (x, y), FNN's goal is to approximate a function $f^*(x)$ via $f(x; \theta)$ where $\theta$ is learned. The approximation is done via assuming that data is generated by  $y\approx f^*(x)$. They consists acyclic graph and of many layers so output of a NN is: $y =f^3(f^2(f^1(x)))$. They are feedforward as there is not feedback loop unlike recurrent neural networks. 

> The training examples specify directly what the output layer must do at each point
> x; it must produce a value that is close to y. The behavior of the other layers is
> not directly specified by the training data. The learning algorithm must decide
> how to use those layers to produce the desired output, but the training data do
> not say what each individual layer should do.   

> It is best to think of
> feedforward networks as function approximation machines that are designed to
> achieve statistical generalization, occasionally drawing some insights from what we
> know about the brain, rather than as models of brain function.  

We can think of DNNs in terms of linear model. A linear models maps input $x$ to output $y$: $y=x^Tw$. However, these models have limited represetational power. Instead of using $x$, we can apply a non-linear transform on $x$: $\phi(x)$. We have three options for $\phi$.

1. Use very generic $\phi$: infinite dimensional that is simiply used by kernel machines. If $\phi$ has high enough dimensions, we can always fit training data. However, generlization remains poor. 
2. Use handcrafted $\phi$ but it takes a lot of time and effort of humans.
3. Use a startegy to learn $\phi$ and then uses it as follows: $y=\phi(x)^Tw$. We can also encode prior knowledge via regulrization. The approach is very generic as we can use families of $\phi$ that we expect to perform good. 



